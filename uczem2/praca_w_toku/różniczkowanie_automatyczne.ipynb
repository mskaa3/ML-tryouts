{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dodatek D – Różniczkowanie automatyczne**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Notatnik ten zawiera przykładowe implementacje różnych technik różniczkowania automatycznego, pozwalające zrozumieć mechanizm ich działania._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Konfiguracja"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wprowadzenie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Załóżmy, że chcemy obliczyć gradienty funkcji $f(x,y)=x^2y + y + 2$ w odniesieniu do parametrów x i y:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x,y):\n",
    "    return x*x*y + y + 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Możemy dokonać tego analitycznie:\n",
    "\n",
    "$\\dfrac{\\partial f}{\\partial x} = 2xy$\n",
    "\n",
    "$\\dfrac{\\partial f}{\\partial y} = x^2 + 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df(x,y):\n",
    "    return 2*x*y, x*x + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zatem, na przykład $\\dfrac{\\partial f}{\\partial x}(3,4) = 24$ i $\\dfrac{\\partial f}{\\partial y}(3,4) = 10$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24, 10)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df(3, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doskonale! Możemy również określić równania dla pochodnych drugiego rzędu (hesjanów):\n",
    "\n",
    "$\\dfrac{\\partial^2 f}{\\partial x \\partial x} = \\dfrac{\\partial (2xy)}{\\partial x} = 2y$\n",
    "\n",
    "$\\dfrac{\\partial^2 f}{\\partial x \\partial y} = \\dfrac{\\partial (2xy)}{\\partial y} = 2x$\n",
    "\n",
    "$\\dfrac{\\partial^2 f}{\\partial y \\partial x} = \\dfrac{\\partial (x^2 + 1)}{\\partial x} = 2x$\n",
    "\n",
    "$\\dfrac{\\partial^2 f}{\\partial y \\partial y} = \\dfrac{\\partial (x^2 + 1)}{\\partial y} = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hesjany te w punkcie x=3 i y=4, wynoszą, odpowiednio, 8, 6, 6, 0. Użyjmy powyższych równań do ich obliczenia:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d2f(x, y):\n",
    "    return [2*y, 2*x], [2*x, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([8, 6], [6, 0])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d2f(3, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Znakomicie, ale rozwiązanie to wymaga nieco zaangażowania matematycznego. W tym przypadku nie było zbyt trudno, ale w przypadku głębokich sieci neuronowych obliczenie pochodnych w ten sposób okazuje się niemal niemożliwe. Zobaczmy więc, jak możemy zautomatyzować ten proces!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Różniczkowanie numeryczne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obliczamy tu wartość przybliżoną gradientów za pomocą równania: $\\dfrac{\\partial f}{\\partial x} = \\displaystyle{\\lim_{\\epsilon \\to 0}}\\dfrac{f(x+\\epsilon, y) - f(x, y)}{\\epsilon}$ (i występuje podobna definicja dla $\\dfrac{\\partial f}{\\partial y}$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradients(func, vars_list, eps=0.0001):\n",
    "    partial_derivatives = []\n",
    "    base_func_eval = func(*vars_list)\n",
    "    for idx in range(len(vars_list)):\n",
    "        tweaked_vars = vars_list[:]\n",
    "        tweaked_vars[idx] += eps\n",
    "        tweaked_func_eval = func(*tweaked_vars)\n",
    "        derivative = (tweaked_func_eval - base_func_eval) / eps\n",
    "        partial_derivatives.append(derivative)\n",
    "    return partial_derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df(x, y):\n",
    "    return gradients(f, [x, y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[24.000400000048216, 10.000000000047748]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df(3, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rozwiązanie to działa dobrze!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dobra wiadomość jest tak, że całkiem łatwo można obliczać w ten sposób hesjany. Stwórzmy najpierw funkcję, która oblicza pochodne pierwszego rzędu (jakobiany): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24.000400000048216, 10.000000000047748)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def dfdx(x, y):\n",
    "    return gradients(f, [x,y])[0]\n",
    "\n",
    "def dfdy(x, y):\n",
    "    return gradients(f, [x,y])[1]\n",
    "\n",
    "dfdx(3., 4.), dfdy(3., 4.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wystarczy teraz użyć funkcji `gradients()` na tych funkcjach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d2f(x, y):\n",
    "    return [gradients(dfdx, [3., 4.]), gradients(dfdy, [3., 4.])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[7.999999951380232, 6.000099261882497],\n",
       " [6.000099261882497, -1.4210854715202004e-06]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d2f(3, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zatem wszystko działa, jak należy, ale wynik jest przybliżeniem, a obliczenie gradientów funkcji w odniesieniu do $n$ zmiennych wymaga wywołania tej funkcji $n$ razy. W głębokich sieciach neuronowych często występują tysiące parametrów, które są modyfikowanie w fazie gradientu prostego (gdy jest wymagane obliczenie gradientów funkcji straty w odniesieniu do każdego z tych parametrów), więc to rozwiązanie jest zbyt powolne."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementacja przykładowego grafu obliczeniowego"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pomińmy rozwiązanie numeryczne i zaimplementujmy pewne technika symbolicznego różniczkowania automatycznego. W tym celu musimy zdefiniować klasy reprezentujące stałe, zmienne i operacje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Const(object):\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "    def evaluate(self):\n",
    "        return self.value\n",
    "    def __str__(self):\n",
    "        return str(self.value)\n",
    "\n",
    "class Var(object):\n",
    "    def __init__(self, name, init_value=0):\n",
    "        self.value = init_value\n",
    "        self.name = name\n",
    "    def evaluate(self):\n",
    "        return self.value\n",
    "    def __str__(self):\n",
    "        return self.name\n",
    "\n",
    "class BinaryOperator(object):\n",
    "    def __init__(self, a, b):\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "\n",
    "class Add(BinaryOperator):\n",
    "    def evaluate(self):\n",
    "        return self.a.evaluate() + self.b.evaluate()\n",
    "    def __str__(self):\n",
    "        return \"{} + {}\".format(self.a, self.b)\n",
    "\n",
    "class Mul(BinaryOperator):\n",
    "    def evaluate(self):\n",
    "        return self.a.evaluate() * self.b.evaluate()\n",
    "    def __str__(self):\n",
    "        return \"({}) * ({})\".format(self.a, self.b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dobrze, możemy teraz stworzyć graf obliczeniowy, reprezentujący funkcję $f$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Var(\"x\")\n",
    "y = Var(\"y\")\n",
    "f = Add(Mul(Mul(x, x), y), Add(y, Const(2))) # f(x,y) = x²y + y + 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Możemy uruchomić ten graf, aby obliczał funkcję $f$ w dowolnym punkcie, na przykład $f(3, 4)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.value = 3\n",
    "y.value = 4\n",
    "f.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Świetnie, uzyskaliśmy ostateczną odpowiedź. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obliczanie gradientów"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poniższe metody różniczkowania automatycznego bazują na *regule łańcuchowej*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Załóżmy, że mamy dwie funkcje, $u$ i $v$, i przekazujemy je sekwencyjnie do jakiegoś wejścia $x$, po czym otrzymujemy wynik $z$. Otrzymujemy więc $z = v(u(x))$, co możemy również zapisać jako $z = v(s)$ i $s = u(x)$. Możemy teraz zastosować regułę łańcuchową, aby uzyskać pochodną cząstkową wyjścia $z$ w odniesieniu do wejścia $x$:\n",
    "\n",
    "$ \\dfrac{\\partial z}{\\partial x} = \\dfrac{\\partial s}{\\partial x} \\cdot \\dfrac{\\partial z}{\\partial s}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jeżeli $z$ jest wynikiem sekwencji funkcji, w której występują wyniki pośrednie $s_1, s_2, ..., s_n$, to reguła łańcuchowa ciągle będzie miała rację bytu: \n",
    "\n",
    "$ \\dfrac{\\partial z}{\\partial x} = \\dfrac{\\partial s_1}{\\partial x} \\cdot \\dfrac{\\partial s_2}{\\partial s_1} \\cdot \\dfrac{\\partial s_3}{\\partial s_2} \\cdot \\dots \\cdot \\dfrac{\\partial s_{n-1}}{\\partial s_{n-2}} \\cdot \\dfrac{\\partial s_n}{\\partial s_{n-1}} \\cdot \\dfrac{\\partial z}{\\partial s_n}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W klasycznym różniczkowaniu automatycznym algorytm oblicza te wyrażenia \"w przód\" (tzn. w tej samej kolejności, co obliczenia wymagane do uzyskania wyniku $z$), czyli od lewej do prawej: najpierw $\\dfrac{\\partial s_1}{\\partial x}$, następnie $\\dfrac{\\partial s_2}{\\partial s_1}$ itd. W odwrotnym różniczkowaniu automatycznym algorytm oblicza te wyrażenia \"wstecz\", od prawej do lewej: najpierw $\\dfrac{\\partial z}{\\partial s_n}$, następnie $\\dfrac{\\partial s_n}{\\partial s_{n-1}}$ itd.\n",
    "\n",
    "Załóżmy, na przykład,że chcesz obliczyć pochodną funkcji $z(x)=\\sin(x^2)$ w punkcie x=3, za pomocą klasycznego różniczkowania automatycznego. Algorytm obliczyłby najpierw pochodną cząstkową $\\dfrac{\\partial s_1}{\\partial x}=\\dfrac{\\partial x^2}{\\partial x}=2x=6$. Następnie przeszedłby do obliczenia $\\dfrac{\\partial z}{\\partial x}=\\dfrac{\\partial s_1}{\\partial x}\\cdot\\dfrac{\\partial z}{\\partial s_1}= 6 \\cdot \\dfrac{\\partial \\sin(s_1)}{\\partial s_1}=6 \\cdot \\cos(s_1)=6 \\cdot \\cos(3^2)\\approx-5,46$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zweryfikujmy ten wynik za pomocą zdefiniowanej wcześniej funkcji `gradients()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-5.46761419430053]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import sin\n",
    "\n",
    "def z(x):\n",
    "    return sin(x**2)\n",
    "\n",
    "gradients(z, [3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wygląda to nieżle. Przeprowadźmy tę samą operację za pomocą odwrotnego różniczkowania automatycznego. Tym razem algorytm rozpocznie z prawej strony i obliczy $\\dfrac{\\partial z}{\\partial s_1} = \\dfrac{\\partial \\sin(s_1)}{\\partial s_1}=\\cos(s_1)=\\cos(3^2)\\approx -0,91$. Następnie zajmie się obliczeniam $\\dfrac{\\partial z}{\\partial x}=\\dfrac{\\partial s_1}{\\partial x}\\cdot\\dfrac{\\partial z}{\\partial s_1} \\approx \\dfrac{\\partial s_1}{\\partial x} \\cdot -0,91 = \\dfrac{\\partial x^2}{\\partial x} \\cdot -0,91=2x \\cdot -0,91 = 6\\cdot-0,91=-5,46$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obydwie metody dają, oczywiście, ten sam wynik (nie licząc błędów zaokrąglenia) i w obecności jednego wejścia oraz wyjścia przeprowadzają identyczną liczbę obliczeń. Jednak w przypadku występowania kilku wejść lub wyjść możemy uzyskiwać bardzo odmienną wydajność. Rzeczywiście, jeżeli występuje wiele wejść, wyrażenia znajdujące się skrajnie po prawej stronie będą musiały obliczać pochodne cząstkowe w odniesieniu do każdego wejścia, dlatego warto je obliczyć jako pierwsze. Oznacza to potrzebę użycia odwrotnego różniczkowania automatycznego. W ten sposób wyrażenia znajdujące się skrajnie po prawej stronie zostaną obliczone tylko i wykorzystanie do obliczenia wszystkich pochodnych cząstkowych. Z drugiej strony, w przypadku występowania wielu wyjść, zazwyczaj lepiej korzystać z klasycznego różniczkowania automatycznego, ponieważ wystarczy obliczyć raz wyrażenia znajdujące się skrajnie po lewej stronie i użyte do obliczenia wszystkich pochodnych cząstkowych dla różnych wyjść. W uczeniu głębokim występują zazwyczaj tysiące parametrów modelu, co oznacza występowania wielu wejść i tylko kilka wyjść. W rzeczywistości, w trakcie uczenia występuje tylko jeden wynik: wartość funkcji straty. Z tego właśnie powodu odwrotne różniczkowanie automatyczne jest używane w module TensorFlow i wszystkich najważniejszych bibliotekach uczenia głębokiego.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W odwrotnym różniczkowaniu automatycznym należy wziąć pod uwagę jeszcze jedną kwestię: wartość $s_i$ jest zazwyczaj wymagana podczas obliczania $\\dfrac{\\partial s_{i+1}}{\\partial s_i}$, natomiast w celu obliczenia $s_i$ należy najpierw obliczyć $s_{i-1}$, a to wymaga obliczenia $s_{i-2}$ itd. Zasadniczo więc pierwszy przebieg w przód przez sieć jest wymagany do obliczenia $s_1$, $s_2$, $s_3$, $\\dots$, $s_{n-1}$ i $s_n$, po czym algorytm może obliczyć pochodne cząstkowe w przeciwnym kierunku. Przechowywanie wszystkich wartości pośrednich $s_i$ w pamięci operacyjnej stanowi czasami problem, zwłaszcza w przypadku przetwarzania obrazów oraz podczas korzystania z karty graficznej, w której pamięć RAM jest nieraz dość ograniczona: aby załagodzić ten problem można zmniejszyć liczbę warstw w sieci neuronowej albo tak skonfigurować moduł TensorFlow, aby wartości te były przenoszone z pamięci operacyjnej karty graficznej do głównej pamięci RAM. Innym rozwiązaniem jest przechowywanie w pamięci podręcznej jedynie co drugiej wartości pośredniej, $s_1$, $s_3$, $s_5$, $\\dots$, $s_{n-4}$, $s_{n-2}$ i $s_n$. Oznacza to, że podczas obliczania pochodnych cząstkowych i braku wartości pośredniej $s_i$, należy ją ponownie obliczyć na podstawie poprzedniej wartości pośredniej $s_{i-1}$. Poświęcamy tu moc obliczeniową na rzecz pamięci operacyjnej (więcej informacji na ten temat znajdziesz w [tym artykule](https://pdfs.semanticscholar.org/f61e/9fd5a4878e1493f7a6b03774a61c17b7e9a4.pdf))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Klasyczne różnicznkowanie automatyczne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Const.gradient = lambda self, var: Const(0)\n",
    "Var.gradient = lambda self, var: Const(1) if self is var else Const(0)\n",
    "Add.gradient = lambda self, var: Add(self.a.gradient(var), self.b.gradient(var))\n",
    "Mul.gradient = lambda self, var: Add(Mul(self.a, self.b.gradient(var)), Mul(self.a.gradient(var), self.b))\n",
    "\n",
    "x = Var(name=\"x\", init_value=3.)\n",
    "y = Var(name=\"y\", init_value=4.)\n",
    "f = Add(Mul(Mul(x, x), y), Add(y, Const(2))) # f(x,y) = x²y + y + 2\n",
    "\n",
    "dfdx = f.gradient(x)  # 2xy\n",
    "dfdy = f.gradient(y)  # x² + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24.0, 10.0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfdx.evaluate(), dfdy.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wynik metody `gradient()` jest całkowicie symboliczny, dlatego nie musimy ograniczać się wyłącznie do pochodnych pierwszego rzędu i możemy obliczać pochodne rzędu drugiego itd.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2fdxdx = dfdx.gradient(x) # 2y\n",
    "d2fdxdy = dfdx.gradient(y) # 2x\n",
    "d2fdydx = dfdy.gradient(x) # 2x\n",
    "d2fdydy = dfdy.gradient(y) # 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[8.0, 6.0], [6.0, 0.0]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[d2fdxdx.evaluate(), d2fdxdy.evaluate()],\n",
    " [d2fdydx.evaluate(), d2fdydy.evaluate()]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zwróć uwagę, że wynik jest teraz dokładny i nie stanowi przybliżenia (oczywiście jedynie ograniczenie stanowi precyzja zmiennoprzecinkowa komputera)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Klasyczne różniczkowanie automatyczne za pomocą liczb dualnych "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ciekawym sposobem stosowania klasycznego różniczkowania automatycznego jest korzystanie z [liczb dualnych](https://pl.wikipedia.org/wiki/Liczby_dualne). Mówiąc krótko, liczba dualna $z$ przyjmuje postać $z = a + b\\epsilon$, gdzie $a$ i $b$ są liczbami rzeczywistymi, natomiast $\\epsilon$ jest nieskończenie małą wartością, dodatnią, ale mniejszą od jakiejkolwiek liczby rzeczywistej, tak że $\\epsilon^2=0$.\n",
    "Można udowodnić, że $f(x + \\epsilon) = f(x) + \\dfrac{\\partial f}{\\partial x}\\epsilon$, zatem wystarczy obliczyć $f(x + \\epsilon)$, aby otrzymać zarówno wartość $f(x)$, jak i pochodną cząstkową $f$ w odniesieniu do $x$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Liczby dualne mają własne reguły arytmetyczne, które są zasadniczo całkiem intuicyjne. Na przykład: \n",
    "\n",
    "**Dodawanie**\n",
    "\n",
    "$(a_1 + b_1\\epsilon) + (a_2 + b_2\\epsilon) = (a_1 + a_2) + (b_1 + b_2)\\epsilon$\n",
    "\n",
    "**Odejmowanie**\n",
    "\n",
    "$(a_1 + b_1\\epsilon) - (a_2 + b_2\\epsilon) = (a_1 - a_2) + (b_1 - b_2)\\epsilon$\n",
    "\n",
    "**Mnożenie**\n",
    "\n",
    "$(a_1 + b_1\\epsilon) \\times (a_2 + b_2\\epsilon) = (a_1 a_2) + (a_1 b_2 + a_2 b_1)\\epsilon + b_1 b_2\\epsilon^2 = (a_1 a_2) + (a_1b_2 + a_2b_1)\\epsilon$\n",
    "\n",
    "**Dzielenie**\n",
    "\n",
    "$\\dfrac{a_1 + b_1\\epsilon}{a_2 + b_2\\epsilon} = \\dfrac{a_1 + b_1\\epsilon}{a_2 + b_2\\epsilon} \\cdot \\dfrac{a_2 - b_2\\epsilon}{a_2 - b_2\\epsilon} = \\dfrac{a_1 a_2 + (b_1 a_2 - a_1 b_2)\\epsilon - b_1 b_2\\epsilon^2}{{a_2}^2 + (a_2 b_2 - a_2 b_2)\\epsilon - {b_2}^2\\epsilon} = \\dfrac{a_1}{a_2} + \\dfrac{a_1 b_2 - b_1 a_2}{{a_2}^2}\\epsilon$\n",
    "\n",
    "**Potęgowanie**\n",
    "\n",
    "$(a + b\\epsilon)^n = a^n + (n a^{n-1}b)\\epsilon$\n",
    "\n",
    "itd."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stwórzmy klasę reprezentującą liczby dualne i zaimplementujmy kilka operacji (dodawanie i mnożenie). Jeśli chcesz, możesz spróbować dołączyć również własne operacje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DualNumber(object):\n",
    "    def __init__(self, value=0.0, eps=0.0):\n",
    "        self.value = value\n",
    "        self.eps = eps\n",
    "    def __add__(self, b):\n",
    "        return DualNumber(self.value + self.to_dual(b).value,\n",
    "                          self.eps + self.to_dual(b).eps)\n",
    "    def __radd__(self, a):\n",
    "        return self.to_dual(a).__add__(self)\n",
    "    def __mul__(self, b):\n",
    "        return DualNumber(self.value * self.to_dual(b).value,\n",
    "                          self.eps * self.to_dual(b).value + self.value * self.to_dual(b).eps)\n",
    "    def __rmul__(self, a):\n",
    "        return self.to_dual(a).__mul__(self)\n",
    "    def __str__(self):\n",
    "        if self.eps:\n",
    "            return \"{:.1f} + {:.1f}ε\".format(self.value, self.eps)\n",
    "        else:\n",
    "            return \"{:.1f}\".format(self.value)\n",
    "    def __repr__(self):\n",
    "        return str(self)\n",
    "    @classmethod\n",
    "    def to_dual(cls, n):\n",
    "        if hasattr(n, \"value\"):\n",
    "            return n\n",
    "        else:\n",
    "            return cls(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$3 + (3 + 4 \\epsilon) = 6 + 4\\epsilon$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.0 + 4.0ε"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3 + DualNumber(3, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$(3 + 4ε)\\times(5 + 7ε)$ = $3 \\times 5 + 3 \\times 7ε + 4ε \\times 5 + 4ε \\times 7ε$ = $15 + 21ε + 20ε + 28ε^2$ = $15 + 41ε + 28 \\times 0$ = $15 + 41ε$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.0 + 41.0ε"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DualNumber(3, 4) * DualNumber(5, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sprawdźmy, czy liczby dual będą współdziałać z naszym przykładowym środowiskiem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.value = DualNumber(3.0)\n",
    "y.value = DualNumber(4.0)\n",
    "\n",
    "f.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tak, zdecydowanie współdziałają. Wykorzystajmy je teraz do obliczenia pochodnych cząstkowych $f$ w odniesieniu do $x$ i $y$ w punktach x=3 i y=4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.value = DualNumber(3.0, 1.0)  # 3 + ε\n",
    "y.value = DualNumber(4.0)       # 4\n",
    "\n",
    "dfdx = f.evaluate().eps\n",
    "\n",
    "x.value = DualNumber(3.0)       # 3\n",
    "y.value = DualNumber(4.0, 1.0)  # 4 + ε\n",
    "\n",
    "dfdy = f.evaluate().eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfdx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfdy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cudownie! Implementacja ta jest jednak ograniczona do pochodnych pierwszego rzędu. \n",
    "Przejdźmy do wariantu odwrotnego."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Odwrotne różniczkowanie automatyczne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zmodyfikujmy nasze środowisko przykładowe i dodajmy odwrotne różniczkowanie automatyczne:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Const(object):\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "    def evaluate(self):\n",
    "        return self.value\n",
    "    def backpropagate(self, gradient):\n",
    "        pass\n",
    "    def __str__(self):\n",
    "        return str(self.value)\n",
    "\n",
    "class Var(object):\n",
    "    def __init__(self, name, init_value=0):\n",
    "        self.value = init_value\n",
    "        self.name = name\n",
    "        self.gradient = 0\n",
    "    def evaluate(self):\n",
    "        return self.value\n",
    "    def backpropagate(self, gradient):\n",
    "        self.gradient += gradient\n",
    "    def __str__(self):\n",
    "        return self.name\n",
    "\n",
    "class BinaryOperator(object):\n",
    "    def __init__(self, a, b):\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "\n",
    "class Add(BinaryOperator):\n",
    "    def evaluate(self):\n",
    "        self.value = self.a.evaluate() + self.b.evaluate()\n",
    "        return self.value\n",
    "    def backpropagate(self, gradient):\n",
    "        self.a.backpropagate(gradient)\n",
    "        self.b.backpropagate(gradient)\n",
    "    def __str__(self):\n",
    "        return \"{} + {}\".format(self.a, self.b)\n",
    "\n",
    "class Mul(BinaryOperator):\n",
    "    def evaluate(self):\n",
    "        self.value = self.a.evaluate() * self.b.evaluate()\n",
    "        return self.value\n",
    "    def backpropagate(self, gradient):\n",
    "        self.a.backpropagate(gradient * self.b.value)\n",
    "        self.b.backpropagate(gradient * self.a.value)\n",
    "    def __str__(self):\n",
    "        return \"({}) * ({})\".format(self.a, self.b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Var(\"x\", init_value=3)\n",
    "y = Var(\"y\", init_value=4)\n",
    "f = Add(Mul(Mul(x, x), y), Add(y, Const(2))) # f(x,y) = x²y + y + 2\n",
    "\n",
    "result = f.evaluate()\n",
    "f.backpropagate(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((x) * (x)) * (y) + y + 2\n"
     ]
    }
   ],
   "source": [
    "print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24.0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Również w tej implementacji wyniki są jedynie wartościami liczbowymi, a nie wyrażeniami symbolicznymi, dlatego jesteśmy ograniczeni do pochodnych pierwszego rzędu. Moglibyśmy jednak sprawić, żeby metody `backpropagate()` zwracały wyrażenia symboliczne zamiast wartości (np. żeby zwracały `Add(2,3)` zamiast 5). W ten sposób moglibyśmy obliczać gradienty drugiego rzędu (i wyższe). To samo robi implementacja w module TensorFlow, jak również wszystkie główne biblioteki zawierające operację różniczkowania automatycznego."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Odwrotne różniczkowanie automatyczne w module TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42.0, [24.0, 10.0])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "x = tf.Variable(3., name=\"x\")\n",
    "y = tf.Variable(4., name=\"y\")\n",
    "f = x*x*y + y + 2\n",
    "\n",
    "jacobians = tf.gradients(f, [x, y])\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    f_val, jacobians_val = sess.run([f, jacobians])\n",
    "\n",
    "f_val, jacobians_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wszystkie wyrażenia są tu symboliczne, dlatego możemy obliczyć pochodne drugiego rzędu i wyższe. Jeżeli jednak obliczymy pochodną tensora w odniesieniu do zmiennej, od której nie jest zależny, to nie otrzymamy w rezultacie 0.0, lecz funkcja `gradients()` zwróci wartość None, której nie można wykorzystać w metodzie `sess.run()`. Uważaj więc na wartości `None`. Zastępujemy je tu tensorami zerowymi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([8.0, 6.0], [6.0, 0.0])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hessians_x = tf.gradients(jacobians[0], [x, y])\n",
    "hessians_y = tf.gradients(jacobians[1], [x, y])\n",
    "\n",
    "def replace_none_with_zero(tensors):\n",
    "    return [tensor if tensor is not None else tf.constant(0.)\n",
    "            for tensor in tensors]\n",
    "\n",
    "hessians_x = replace_none_with_zero(hessians_x)\n",
    "hessians_y = replace_none_with_zero(hessians_y)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    hessians_x_val, hessians_y_val = sess.run([hessians_x, hessians_y])\n",
    "\n",
    "hessians_x_val, hessians_y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I to by było na tyle! Mam nadzieję, że spodobał Ci się ten notatnik."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "nav_menu": {
   "height": "603px",
   "width": "616px"
  },
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
